import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.7.1'
import { analyzeTaskIntent } from "./taskParsing.ts";
import { processWithBuddyChatAI } from "./chatAnalysis.ts";
import { generateImageWithRunware } from "./imageGeneration.ts";
import { executeRegularSearch } from "./search.ts";
import { generateConversationId, DEEPSEEK_API_KEY, OPENAI_API_KEY, TAVILY_API_KEY, RUNWARE_API_KEY, supabase } from "./utils.ts";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type, x-app-name, x-auth-token, x-skip-auth',
  'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
};

const SUPABASE_URL = Deno.env.get('SUPABASE_URL')!;
const SUPABASE_ANON_KEY = Deno.env.get('SUPABASE_ANON_KEY')!;
const OPENAI_API_KEY = Deno.env.get('OPENAI_API_KEY');
const DEEPSEEK_API_KEY = Deno.env.get('DEEPSEEK_API_KEY');

const supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY);

console.log("üîç UNIFIED AI BRAIN: Function loaded with optimized voice translation and instant TTS");

serve(async (req) => {
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    console.log("üîç UNIFIED AI BRAIN: Processing request with optimized TTS");
    
    const contentType = req.headers.get('content-type') || '';
    
    // Handle voice translation with audio blob (FormData)
    if (contentType.includes('multipart/form-data')) {
      console.log("üé§ VOICE TRANSLATION: Processing audio blob with auto-TTS");
      return await processVoiceTranslationWithAutoTTS(req);
    }
    
    // Handle TTS-only requests (JSON)
    const requestBody = await req.json();
    
    if (requestBody.requestType === 'tts' || requestBody.text) {
      console.log("üîä TTS REQUEST DETECTED:", requestBody.voice || 'alloy', "Body:", JSON.stringify(requestBody, null, 2));
      return await processTTSOptimized(requestBody);
    }

    // Handle regular AI requests
    return await processRegularAIRequest(requestBody, req);

  } catch (error) {
    console.error("üö® UNIFIED AI BRAIN: Critical Error:", error);
    return new Response(JSON.stringify({
      error: error.message || 'Processing error',
      success: false
    }), {
      status: 500,
      headers: { ...corsHeaders, "Content-Type": "application/json" }
    });
  }
});

// OPTIMIZED: Voice translation with automatic TTS generation
async function processVoiceTranslationWithAutoTTS(req: Request) {
  try {
    console.log("üé§ VOICE TRANSLATION: Starting optimized processing");
    
    const formData = await req.formData();
    const audioBlob = formData.get('audioBlob') as File;
    const targetLanguage = formData.get('targetLanguage') as string || 'en';
    const autoPlayEnabled = formData.get('autoPlayEnabled') === 'true';
    
    if (!audioBlob) {
      throw new Error('No audio blob provided');
    }

    console.log("üé§ Processing audio blob:", audioBlob.size, "bytes for language:", targetLanguage);

    // Step 1: Transcribe audio using Whisper
    const audioBuffer = await audioBlob.arrayBuffer();
    const transcriptionResult = await transcribeWithWhisper(audioBuffer);
    
    if (!transcriptionResult.success) {
      throw new Error(transcriptionResult.error || 'Transcription failed');
    }

    const originalText = transcriptionResult.text;
    console.log("üé§ Transcribed text:", originalText);

    // Step 2: Translate if needed
    let translatedText = originalText;
    let sourceLanguage = transcriptionResult.language || 'auto';
    
    if (targetLanguage !== 'auto' && sourceLanguage !== targetLanguage) {
      const translationResult = await translateText(originalText, sourceLanguage, targetLanguage);
      if (translationResult.success) {
        translatedText = translationResult.translatedText;
        console.log("üé§ Translated to:", translatedText);
      }
    }

    // Step 3: AUTOMATICALLY generate TTS for instant playback
    let autoGeneratedTTS = null;
    if (translatedText) {
      console.log("üîä AUTO-GENERATING TTS for instant playback");
      try {
        const ttsResult = await generateTTSInstantly(translatedText, targetLanguage);
        if (ttsResult.success) {
          autoGeneratedTTS = {
            audioContent: ttsResult.audioContent,
            size: ttsResult.size || 0
          };
          console.log("üîä TTS auto-generated successfully for instant playback");
        }
      } catch (ttsError) {
        console.error("üîä Auto-TTS generation failed (non-blocking):", ttsError);
      }
    }

    // Return structured response with auto-generated TTS
    const response = {
      success: true,
      originalText,
      translatedText,
      sourceLanguage,
      targetLanguage,
      autoGeneratedTTS, // Pre-generated for instant playback
      autoPlayEnabled,
      processingTime: Date.now()
    };

    console.log("üé§ VOICE TRANSLATION: Completed with auto-TTS");
    return new Response(JSON.stringify(response), {
      headers: { ...corsHeaders, "Content-Type": "application/json" }
    });

  } catch (error) {
    console.error("üé§ VOICE TRANSLATION ERROR:", error);
    return new Response(JSON.stringify({
      success: false,
      error: error.message || 'Voice translation failed'
    }), {
      status: 500,
      headers: { ...corsHeaders, "Content-Type": "application/json" }
    });
  }
}

// OPTIMIZED: Single TTS function with no recursion
async function processTTSOptimized(requestBody: any) {
  try {
    console.log("üîä OPTIMIZED TTS: Processing instant text-to-speech for user:", requestBody.userId || 'unknown');
    
    const text = requestBody.text;
    const voice = requestBody.voice || 'alloy';
    const language = requestBody.language || 'en';
    
    if (!text) {
      throw new Error('No text provided for TTS');
    }

    console.log(`üîä OPTIMIZED TTS: Generating instant speech for text: "${text.substring(0, 100)}..."`);
    
    const ttsResult = await generateTTSInstantly(text, language, voice);
    
    if (!ttsResult.success) {
      throw new Error(ttsResult.error || 'TTS generation failed');
    }

    console.log("üîä OPTIMIZED TTS: Generated successfully, size:", ttsResult.size || 0, "bytes");

    return new Response(JSON.stringify({
      success: true,
      audioContent: ttsResult.audioContent,
      size: ttsResult.size || 0,
      voice,
      language,
      text: text.substring(0, 100)
    }), {
      headers: { ...corsHeaders, "Content-Type": "application/json" }
    });

  } catch (error) {
    console.error("üîä OPTIMIZED TTS: Error:", error);
    return new Response(JSON.stringify({
      success: false,
      error: error.message || 'TTS processing failed'
    }), {
      status: 500,
      headers: { ...corsHeaders, "Content-Type": "application/json" }
    });
  }
}

// FAST: Generate TTS instantly with OpenAI
async function generateTTSInstantly(text: string, language: string = 'en', voice: string = 'alloy'): Promise<any> {
  try {
    if (!OPENAI_API_KEY) {
      throw new Error('OpenAI API key not configured');
    }

    console.log("üîä TTS: Calling OpenAI TTS API directly");
    
    const response = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'tts-1-hd',
        input: text.substring(0, 4000), // Limit text length
        voice: voice,
        response_format: 'mp3'
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("üîä TTS: OpenAI API error:", response.status, errorText);
      throw new Error(`OpenAI TTS API error: ${response.status}`);
    }

    const audioBuffer = await response.arrayBuffer();
    const audioContent = btoa(String.fromCharCode(...new Uint8Array(audioBuffer)));
    
    console.log("üîä TTS: Generated audio successfully, size:", audioBuffer.byteLength, "bytes");

    return {
      success: true,
      audioContent,
      size: audioBuffer.byteLength
    };

  } catch (error) {
    console.error("üîä TTS: Generation error:", error);
    return {
      success: false,
      error: error.message
    };
  }
}

// Helper: Transcribe audio with Whisper
async function transcribeWithWhisper(audioBuffer: ArrayBuffer): Promise<any> {
  try {
    if (!OPENAI_API_KEY) {
      throw new Error('OpenAI API key not configured');
    }

    const formData = new FormData();
    const audioBlob = new Blob([audioBuffer], { type: 'audio/webm' });
    formData.append('file', audioBlob, 'audio.webm');
    formData.append('model', 'whisper-1');
    formData.append('response_format', 'verbose_json');

    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`Whisper API error: ${response.status}`);
    }

    const result = await response.json();
    
    return {
      success: true,
      text: result.text,
      language: result.language || 'en'
    };

  } catch (error) {
    console.error("üé§ Whisper transcription error:", error);
    return {
      success: false,
      error: error.message
    };
  }
}

// Helper: Translate text
async function translateText(text: string, sourceLanguage: string, targetLanguage: string): Promise<any> {
  try {
    if (!OPENAI_API_KEY) {
      throw new Error('OpenAI API key not configured');
    }

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: `Translate the following text from ${sourceLanguage} to ${targetLanguage}. Return only the translation, nothing else.`
          },
          {
            role: 'user',
            content: text
          }
        ],
        max_tokens: 1000,
        temperature: 0.1
      }),
    });

    if (!response.ok) {
      throw new Error(`Translation API error: ${response.status}`);
    }

    const result = await response.json();
    const translatedText = result.choices[0]?.message?.content?.trim();

    if (!translatedText) {
      throw new Error('No translation received');
    }

    return {
      success: true,
      translatedText
    };

  } catch (error) {
    console.error("üåç Translation error:", error);
    return {
      success: false,
      error: error.message
    };
  }
}

// Handle regular AI requests (existing functionality)
async function processRegularAIRequest(requestBody: any, req: Request) {
  const skipAuth = req.headers.get('x-skip-auth') === 'true';
    const authToken = req.headers.get('x-auth-token');
    
    let user;
    if (skipAuth && authToken) {
      try {
        const { data } = await supabase.auth.getUser(authToken);
        user = data.user;
      } catch (e) {
        const authHeader = req.headers.get('authorization');
        if (!authHeader) throw new Error('Authentication required');
        const { data } = await supabase.auth.getUser(authHeader.replace('Bearer ', ''));
        user = data.user;
      }
    } else {
      const authHeader = req.headers.get('authorization');
      if (!authHeader) throw new Error('Authentication required');
      const { data } = await supabase.auth.getUser(authHeader.replace('Bearer ', ''));
      user = data.user;
    }

    if (!user) {
      return new Response(JSON.stringify({ 
        error: "Invalid authentication",
        success: false
      }), {
        status: 401,
        headers: { ...corsHeaders, "Content-Type": "application/json" }
      });
    }

    const {
      message,
      userId,
      language = 'en',
      conversationId = null,
      inputType = 'text',
      activeTrigger = 'chat',
      attachedFiles = [],
      conversationSummary = '',
      recentMessages = [],
      customSystemPrompt = '',
      maxTokens = 400,
      userStyle = 'detailed',
      userTone = 'neutral',
      speedOptimized = true,
      aggressiveOptimization = true,
      hasTaskIntent = false,
      personalityEnabled = true,
      enableTaskCreation = true,
      enablePersonality = true,
      personalTouch = null
    } = requestBody;

    if (userId !== user.id) {
      return new Response(JSON.stringify({ 
        error: "User ID mismatch",
        success: false
      }), {
        status: 403,
        headers: { ...corsHeaders, "Content-Type": "application/json" }
      });
    }

    if (!message?.trim() && !attachedFiles?.length) {
      return new Response(JSON.stringify({ 
        error: "Message or attachment required",
        success: false
      }), {
        status: 400,
        headers: { ...corsHeaders, "Content-Type": "application/json" }
      });
    }

    console.log(`üöÄ ULTRA-FAST AI: User ${user.id} | Personal Touch: ${!!personalTouch} | Speed Mode: ${speedOptimized} | Aggressive: ${aggressiveOptimization}`);

    // ULTRA-FAST: Process attached files with minimal overhead (ENHANCED for Vision)
    let processedFiles = [];
    if (attachedFiles && attachedFiles.length > 0) {
      processedFiles = await processAttachedFilesOptimized(attachedFiles);
      console.log(`üöÄ ULTRA-FAST: Processed ${processedFiles.length} files (Vision-ready)`);
    }

    // ULTRA-FAST: Minimal context for maximum speed
    let minimalRecentMessages = aggressiveOptimization ? recentMessages.slice(-2) : recentMessages.slice(-3);
    let minimalConversationSummary = aggressiveOptimization ? '' : conversationSummary.substring(0, 200);
    
    console.log(`üöÄ SPEED MODE: Context messages: ${minimalRecentMessages.length}, Summary: ${minimalConversationSummary.length} chars`);

    // ENHANCED: Task detection for ALL chat triggers (not just when enableTaskCreation)
    let taskAnalysisResult = null;
    try {
      console.log("üîç TASK DETECTION: Analyzing message for task intent");
      taskAnalysisResult = await analyzeTaskIntent(message, language);
      console.log("üîç TASK ANALYSIS RESULT:", JSON.stringify(taskAnalysisResult, null, 2));
    } catch (taskError) {
      console.error("üîç TASK ANALYSIS ERROR:", taskError);
    }

    // CRITICAL FIX: Return structured confirmation data when task is detected
    if (taskAnalysisResult && (taskAnalysisResult.isTask || taskAnalysisResult.isReminder)) {
      console.log(`üîç TASK DETECTED: ${taskAnalysisResult.isTask ? 'Task' : 'Reminder'} - Returning confirmation data`);
      
      const startTime = Date.now();
      
      // Return structured confirmation response (NO text response)
      const result = {
        response: '', // Empty response - let the UI handle the confirmation
        conversationId: conversationId || generateConversationId(),
        intent: taskAnalysisResult.isTask ? 'task_creation' : 'reminder_creation',
        confidence: 'high',
        actionTaken: false,
        imageUrl: null,
        browsingUsed: false,
        browsingData: null,
        needsConfirmation: true, // CRITICAL: This triggers the confirmation form
        pendingTaskData: taskAnalysisResult.isTask ? taskAnalysisResult.taskData : null,
        pendingReminderData: taskAnalysisResult.isReminder ? taskAnalysisResult.reminderData : null,
        success: true,
        processingTime: startTime - Date.now(),
        speedOptimized: true,
        aggressiveOptimization,
        userStyle,
        userTone,
        tokensUsed: 0, // No AI tokens used for task detection
        aiProvider: 'task_parser',
        taskCreationEnabled: true,
        personalizedResponse: false,
        taskDetected: true,
        ultraFastMode: {
          speedOptimized,
          aggressiveOptimization,
          contextMessages: 0,
          summaryLength: 0,
          tokensLimit: 0,
          personalTouch: false
        }
      };

      console.log(`üöÄ TASK CONFIRMATION: Returning structured data in ${startTime - Date.now()}ms`);
      return new Response(JSON.stringify(result), {
        headers: { ...corsHeaders, "Content-Type": "application/json" }
      });
    }

    // ULTRA-FAST: Main processing with timeout protection (only if no task detected)
    let response = '';
    let imageUrl = null;
    let browsingUsed = false;
    let browsingData = null;
    let actionTaken = null;

    switch (activeTrigger) {
      case 'search':
        if (!aggressiveOptimization) {
          console.log("üîç FAST SEARCH: Speed-optimized search");
          const searchResult = await executeRegularSearch(message, language);
          if (searchResult.success) {
            browsingUsed = true;
            browsingData = searchResult.data;
            const context = searchResult.context.substring(0, aggressiveOptimization ? 300 : 800);
            
            // CRITICAL FIX: Extract response field from result object
            const chatResult = await processWithBuddyChatAI(
              `${message}\n\nSearch Context: ${context}`,
              userId,
              conversationId,
              language,
              processedFiles,
              minimalRecentMessages,
              minimalConversationSummary,
              personalTouch,
              Math.min(maxTokens, 300),
              activeTrigger
            );
            response = chatResult.response; // Extract the response field
          } else {
            // CRITICAL FIX: Extract response field from result object
            const chatResult = await processWithBuddyChatAI(
              message,
              userId,
              conversationId,
              language,
              processedFiles,
              [],
              '',
              personalTouch,
              Math.min(maxTokens, 200),
              activeTrigger
            );
            response = chatResult.response; // Extract the response field
          }
        } else {
          // CRITICAL FIX: Extract response field from result object
          const chatResult = await processWithBuddyChatAI(
            message,
            userId,
            conversationId,
            language,
            processedFiles,
            [],
            '',
            personalTouch,
            Math.min(maxTokens, 150),
            'chat'
          );
          response = chatResult.response; // Extract the response field
        }
        break;

      case 'image':
        if (!aggressiveOptimization) {
          console.log("üé® FAST IMAGE: Speed-optimized image generation");
          try {
            const imageResult = await generateImageWithRunware(message, user.id, language);
            
            if (imageResult.success) {
              imageUrl = imageResult.imageUrl;
              
              let baseResponse = language === 'ar' 
                ? `ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑÿµŸàÿ±ÿ© ÿ®ŸÜÿ¨ÿßÿ≠! üé®‚ú®`
                : `Image generated successfully! üé®‚ú®`;

              if (imageResult.translation_status === 'success' && imageResult.translatedPrompt) {
                baseResponse += language === 'ar'
                  ? `\n\nüìù (ÿ™ÿ±ÿ¨ŸÖÿ©: "${imageResult.translatedPrompt}")`
                  : `\n\nüìù (Translated: "${imageResult.translatedPrompt}")`;
              }

              response = baseResponse;
            } else {
              response = imageResult.error;
            }
          } catch (error) {
            console.error("Fast image generation error:", error);
            response = language === 'ar' 
              ? `‚ùå ÿπÿ∞ÿ±ÿßŸãÿå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑÿµŸàÿ±ÿ©.`
              : `‚ùå Sorry, an error occurred while generating the image.`;
          }
        } else {
          response = language === 'ar' 
            ? `ÿπÿ∞ÿ±ÿßŸãÿå ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑÿµŸàÿ± ÿ∫Ÿäÿ± ŸÖÿ™ÿßÿ≠ ŸÅŸä ÿßŸÑŸàÿ∂ÿπ ÿßŸÑÿ≥ÿ±Ÿäÿπ.`
            : `Sorry, image generation not available in ultra-fast mode.`;
        }
        break;

      case 'chat':
      default:
        console.log(`üöÄ ULTRA-FAST CHAT: Processing with timeout protection and personalization`);
        
        // ULTRA-FAST: Minimal context for lightning speed
        let chatContext = aggressiveOptimization ? '' : minimalConversationSummary;
        
        console.log(`üöÄ ULTRA-FAST CHAT: Context: ${chatContext?.length || 0} | Messages: ${minimalRecentMessages.length} | Personal Touch: ${!!personalTouch}`);
        
        // CRITICAL FIX: Extract response field from result object
        const chatResult = await processWithBuddyChatAI(
          message,
          userId,
          conversationId,
          language,
          processedFiles,
          minimalRecentMessages,
          chatContext,
          personalTouch,
          maxTokens,
          activeTrigger
        );
        response = chatResult.response; // Extract the response field
        break;
    }

    const processingTime = Date.now() - startTime;
    console.log(`üöÄ ULTRA-FAST: Processed in ${processingTime}ms (${aggressiveOptimization ? 'HYPER-FAST' : speedOptimized ? 'ULTRA-FAST' : 'SPEED'} mode)`);

    // ULTRA-FAST: Response structure optimized for speed
    const result = {
      response,
      conversationId: conversationId || generateConversationId(),
      intent: aggressiveOptimization ? 'hyper_fast' : (speedOptimized ? 'ultra_fast' : 'speed_optimized'),
      confidence: 'high',
      actionTaken,
      imageUrl,
      browsingUsed,
      browsingData,
      needsConfirmation: false, // No confirmation needed for regular chat
      pendingTaskData: null,
      pendingReminderData: null,
      success: true,
      processingTime,
      speedOptimized: true,
      aggressiveOptimization,
      userStyle,
      userTone,
      tokensUsed: maxTokens,
      aiProvider: OPENAI_API_KEY ? 'openai' : 'deepseek',
      taskCreationEnabled: enableTaskCreation,
      personalizedResponse: !!personalTouch,
      ultraFastMode: {
        speedOptimized,
        aggressiveOptimization,
        contextMessages: minimalRecentMessages.length,
        summaryLength: minimalConversationSummary.length,
        tokensLimit: maxTokens,
        personalTouch: !!personalTouch
      }
    };

    return new Response(JSON.stringify(result), {
      headers: { ...corsHeaders, "Content-Type": "application/json" }
    });
}

// HYPER-OPTIMIZED: Process files with URL handling for Vision API
async function processAttachedFilesOptimized(attachedFiles: any[]): Promise<any[]> {
  if (!attachedFiles || attachedFiles.length === 0) return [];

  return attachedFiles.map(file => {
    // ENHANCED: For Vision API, we need the public URL
    if (file.type && file.type.startsWith('image/')) {
      // If file is optimized with public URL, use it directly for Vision
      if (file.optimized && file.publicUrl) {
        console.log("üîç VISION: Using optimized public URL for Vision API");
        return {
          type: 'image',
          publicUrl: file.publicUrl,
          optimized: true,
          ...file
        };
      }
      
      // If we have a regular URL, use it
      if (file.url) {
        console.log("üîç VISION: Using regular URL for Vision API");
        return {
          type: 'image',
          url: file.url,
          ...file
        };
      }
    }
    
    // Fallback to existing Base64 processing for non-Vision files
    if (file.content) {
      return {
        type: 'image_url',
        image_url: {
          url: `data:${file.type};base64,${file.content}`
        }
      };
    }
    
    return null;
  }).filter(Boolean);
}
