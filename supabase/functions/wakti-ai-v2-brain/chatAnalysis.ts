
// Remove the old OpenAI import and use direct fetch calls instead
import { generateTaskPrompt } from "./taskAnalysisPrompts.ts";
import { analyzeTaskIntent } from "./taskAnalysis.ts";

const OPENAI_API_KEY = Deno.env.get("OPENAI_API_KEY");

// System prompts
const englishSystemPrompt = `You are a helpful AI assistant that analyzes images. Extract all visible text, identify people, objects, and scenes, and respond with clear, structured descriptions. Always reason from visual details and provide insights where possible.`;

const arabicSystemPrompt = `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ÙÙŠØ¯ ÙŠØ­Ù„Ù„ Ø§Ù„ØµÙˆØ±. Ø§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø¦ÙŠØŒ ÙˆØ­Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø®Ø§Øµ ÙˆØ§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙˆØ§Ù„Ù…Ø´Ø§Ù‡Ø¯ØŒ ÙˆØ§Ø³ØªØ¬Ø¨ Ø¨ÙˆØµÙ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø¸Ù…. Ø§Ø³ØªØ¯Ù„ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙˆÙ‚Ø¯Ù… Ø±Ø¤Ù‰ Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù…ÙƒØ§Ù†.`;

// Vision prompts
const englishVisionPromptTemplate = `Analyze the image and provide a detailed description. Identify any objects, people, or scenes present. Extract any text or information that can be read from the image.`;
const arabicVisionPromptTemplate = `Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø© ÙˆÙ‚Ø¯Ù… ÙˆØµÙÙ‹Ø§ ØªÙØµÙŠÙ„ÙŠÙ‹Ø§. Ø­Ø¯Ø¯ Ø£ÙŠ ÙƒØ§Ø¦Ù†Ø§Øª Ø£Ùˆ Ø£Ø´Ø®Ø§Øµ Ø£Ùˆ Ù…Ø´Ø§Ù‡Ø¯ Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø§Ø³ØªØ®Ø±Ø¬ Ø£ÙŠ Ù†Øµ Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©.`;

// Image compression utility function
async function compressImageForVision(imageUrl: string): Promise<string> {
  try {
    console.log('ğŸ” COMPRESSION: Starting image compression for Vision API');
    
    // Fetch the image
    const response = await fetch(imageUrl);
    if (!response.ok) {
      throw new Error(`Failed to fetch image: ${response.status}`);
    }
    
    const arrayBuffer = await response.arrayBuffer();
    const uint8Array = new Uint8Array(arrayBuffer);
    
    // For Deno edge functions, we'll use a simplified compression approach
    // Convert to base64 with size limits
    const base64 = btoa(String.fromCharCode(...uint8Array));
    
    // Apply size limits (max 1MB for Vision API)
    const maxSize = 1024 * 1024; // 1MB limit
    if (base64.length > maxSize) {
      console.log('ğŸ” COMPRESSION: Image too large, applying compression');
      // Simple truncation-based compression approach for Deno
      return base64.substring(0, maxSize);
    }
    
    console.log('ğŸ” COMPRESSION: Image compression completed');
    return base64;
  } catch (error) {
    console.error('ğŸš¨ COMPRESSION ERROR:', error);
    // Return original URL if compression fails
    return imageUrl;
  }
}

// Build vision system prompt based on language
function buildVisionSystemPrompt(language: string): string {
  return language === 'ar' ? arabicSystemPrompt : englishSystemPrompt;
}

// Get vision prompt template based on image type and language
function getVisionPromptTemplate(imageType: string, language: string): string {
  let prompt = language === 'ar' ? arabicVisionPromptTemplate : englishVisionPromptTemplate;
  
  if (imageType === 'document') {
    prompt += language === 'ar'
      ? '\n\nØ§Ø³ØªØ®Ø±Ø¬ Ø£ÙŠ Ù†Øµ Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø¬Ø¯Ø§ÙˆÙ„ Ø£Ùˆ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†Ø¸Ù…Ø©ØŒ ÙÙ‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ù†Ø¸Ù….'
      : '\n\nExtract any text or information that can be read from the image. If there are any tables or structured data, present them in an organized format.';
  }
  
  return prompt;
}

// Detect image type based on message content
function detectImageType(message: string): string {
  const lowerMessage = message.toLowerCase();
  if (lowerMessage.includes('document') || lowerMessage.includes('invoice') || lowerMessage.includes('receipt')) {
    return 'document';
  }
  return 'general';
}

export async function processWithBuddyChatAI(
  message: string,
  userId: string,
  conversationId: string | null,
  language: string = 'en',
  attachedFiles: any[] = [],
  recentMessages: any[] = [],
  conversationSummary: string = '',
  personalTouch: any = null,
  maxTokens: number = 500,
  activeTrigger: string = 'chat'
): Promise<any> {
  try {
    console.log('ğŸš€ ULTRA-FAST CHAT: Processing with timeout protection and personalization');
    
    const startTime = Date.now();
    
    // Task detection (non-blocking)
    let isTask = false;
    let taskData = null;
    
    try {
      const taskAnalysis = await analyzeTaskIntent(message, language);
      isTask = taskAnalysis.isTask;
      taskData = taskAnalysis.taskData;
      
      if (isTask && taskData) {
        console.log('âœ… TASK DETECTED:', taskData);
      } else {
        console.log('â„¹ï¸  No task detected');
      }
    } catch (taskError) {
      console.warn('âš ï¸  Task analysis failed:', taskError);
    }
    
    // Personalization setup
    let personalizedTemperature = 0.7;
    let systemPrompt = language === 'ar' ? arabicSystemPrompt : englishSystemPrompt;
    
    if (personalTouch) {
      console.log('ğŸ¨ PERSONALIZATION: Applying personal touch settings');
      
      if (personalTouch.tone) {
        console.log('   - Setting tone:', personalTouch.tone);
        if (personalTouch.tone === 'formal') {
          personalizedTemperature = Math.max(0.3, personalizedTemperature - 0.2);
        } else if (personalTouch.tone === 'friendly') {
          personalizedTemperature = Math.min(0.9, personalizedTemperature + 0.2);
        }
      }
      
      if (personalTouch.instruction) {
        console.log('   - Adding custom instruction:', personalTouch.instruction);
        systemPrompt += `\n\nADDITIONAL INSTRUCTION: ${personalTouch.instruction}`;
      }
      
      if (personalTouch.aiNickname) {
        console.log('   - Setting AI nickname:', personalTouch.aiNickname);
        systemPrompt = systemPrompt.replace(/WAKTI/g, personalTouch.aiNickname);
      }
    }
    
    // Vision processing with compression
    if (attachedFiles && attachedFiles.length > 0) {
      console.log('ğŸ” VISION MODE: Processing', attachedFiles.length, 'attached files');
      console.log('ğŸ” VISION PROCESSING: Starting OpenAI Vision analysis with enhanced prompts');

      const visionSystemPrompt = buildVisionSystemPrompt(language);
      
      // Process files with compression
      const processedFiles = [];
      for (const file of attachedFiles) {
        if (file.publicUrl) {
          console.log('ğŸ” VISION: Compressing image before API call');
          const compressedBase64 = await compressImageForVision(file.publicUrl);
          
          // Use compressed base64 format
          processedFiles.push({
            type: "image_url",
            image_url: {
              url: compressedBase64.startsWith('data:') 
                ? compressedBase64 
                : `data:image/jpeg;base64,${compressedBase64}`
            }
          });
          console.log('ğŸ” VISION: Added compressed image for analysis');
        }
      }

      // Detect image type and enhance message
      const imageType = detectImageType(message);
      console.log('ğŸ” VISION: Detected image type:', imageType);
      
      const visionPromptTemplate = getVisionPromptTemplate(imageType, language);
      const enhancedMessage = `${message}\n\n${visionPromptTemplate}`;
      console.log('ğŸ” VISION: Enhanced message:', enhancedMessage.substring(0, 100) + '...');

      // Use current OpenAI vision model
      console.log('ğŸ” VISION API: Calling OpenAI Vision with gpt-4o-2024-05-13');
      
      const visionResponse = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${OPENAI_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-4o-2024-05-13', // Updated to current vision model
          messages: [
            { role: 'system', content: visionSystemPrompt },
            {
              role: 'user',
              content: [
                { type: 'text', text: enhancedMessage },
                ...processedFiles
              ]
            }
          ],
          max_tokens: Math.max(maxTokens * 2, 800), // More tokens for vision analysis
          temperature: personalizedTemperature
        }),
      });

      if (!visionResponse.ok) {
        const errorData = await visionResponse.json();
        console.error('ğŸš¨ VISION API ERROR:', errorData);
        throw new Error(`Vision API error: ${visionResponse.status}`);
      }

      const visionData = await visionResponse.json();
      const visionResult = visionData.choices[0].message.content;

      console.log('âœ… VISION SUCCESS:', visionResult.length, 'characters');

      return {
        response: visionResult,
        success: true,
        model: 'gpt-4o-2024-05-13-vision',
        processingTime: Date.now() - startTime,
        tokensUsed: visionData.usage?.total_tokens || 0,
        visionProcessed: true,
        personalizedResponse: !!personalTouch
      };
    }

    // Regular chat processing with OpenAI
    console.log('ğŸ’¬ REGULAR CHAT: Calling OpenAI with gpt-4o-mini');
    
    const openaiResponse = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: message }
        ],
        max_tokens: maxTokens,
        temperature: personalizedTemperature
      }),
    });

    if (!openaiResponse.ok) {
      const errorData = await openaiResponse.json();
      console.error('ğŸš¨ OPENAI ERROR:', errorData);
      throw new Error(`OpenAI API error: ${openaiResponse.status}`);
    }

    const openaiData = await openaiResponse.json();
    const aiResponse = openaiData.choices[0].message.content;

    console.log('âœ… OPENAI SUCCESS:', aiResponse.length, 'characters');

    return {
      response: aiResponse,
      success: true,
      model: 'gpt-4o-mini',
      processingTime: Date.now() - startTime,
      tokensUsed: openaiData.usage?.total_tokens || 0,
      personalizedResponse: !!personalTouch,
      isTask: isTask,
      taskData: taskData
    };

  } catch (error) {
    console.error('ğŸš¨ ENHANCED CHAT ERROR:', error);
    throw error;
  }
}
