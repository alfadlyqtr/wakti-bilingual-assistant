
/**
 * Enhanced chat analysis with ultra-fast processing and post-processing personalization
 */
import { DEEPSEEK_API_KEY, OPENAI_API_KEY } from "./utils.ts";

export function analyzeBuddyChatIntent(message: string, activeTrigger: string, enhancedContext: string, language: string = 'en') {
  const lowerMessage = message.toLowerCase();
  
  // ENHANCED: Better pattern matching for personality
  const patterns = {
    greeting: ['hi', 'hello', 'hey', 'Ù…Ø±Ø­Ø¨Ø§', 'Ø£Ù‡Ù„Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…'],
    question: ['what', 'how', 'when', 'why', 'where', 'Ù…Ø§', 'ÙƒÙŠÙ', 'Ù…ØªÙ‰', 'Ù„Ù…Ø§Ø°Ø§', 'Ø£ÙŠÙ†', '?'],
    thanks: ['thanks', 'thank', 'Ø´ÙƒØ±Ø§', 'Ø´ÙƒØ±Ø§Ù‹'],
    task: ['create task', 'add task', 'Ø£Ù†Ø´Ø¦ Ù…Ù‡Ù…Ø©', 'Ø£Ø¶Ù Ù…Ù‡Ù…Ø©'],
    reminder: ['remind me', 'set reminder', 'Ø°ÙƒØ±Ù†ÙŠ', 'Ø£Ù†Ø´Ø¦ ØªØ°ÙƒÙŠØ±']
  };
  
  let intent = 'general_chat';
  let confidence = 'medium';
  
  // Enhanced intent detection
  for (const [intentType, patternList] of Object.entries(patterns)) {
    if (patternList.some(pattern => lowerMessage.includes(pattern))) {
      intent = intentType;
      confidence = 'high';
      break;
    }
  }
  
  return {
    intent,
    confidence,
    naturalQuery: true,
    conversational: true
  };
}

export function analyzeSmartModeIntent(message: string, activeTrigger: string, language: string = 'en') {
  const lowerMessage = message.toLowerCase();
  
  // ENHANCED: Better mode detection with personality
  const modePatterns = {
    search: ['weather', 'news', 'price', 'search for', 'Ø·Ù‚Ø³', 'Ø£Ø®Ø¨Ø§Ø±', 'Ø§Ø¨Ø­Ø« Ø¹Ù†'],
    image: ['create image', 'draw', 'generate image', 'Ø£Ù†Ø´Ø¦ ØµÙˆØ±Ø©', 'Ø§Ø±Ø³Ù…'],
    chat: ['chat', 'talk', 'conversation', 'ØªØ­Ø¯Ø«', 'Ù…Ø­Ø§Ø¯Ø«Ø©']
  };
  
  let suggestMode = null;
  let allowInMode = true;
  
  // Enhanced mode suggestion
  if (activeTrigger === 'chat') {
    for (const [mode, patterns] of Object.entries(modePatterns)) {
      if (mode !== 'chat' && patterns.some(pattern => lowerMessage.includes(pattern))) {
        suggestMode = mode;
        break;
      }
    }
  }
  
  return {
    suggestMode,
    allowInMode,
    naturalFlow: true
  };
}

// ENHANCED: Build personalized system prompts BEFORE API calls
const buildPersonalizedSystemPrompt = (
  language: string,
  personalTouch: any | null,
  interactionType: string
) => {
  let systemPrompt = language === 'ar' 
    ? 'Ø£Ù†Øª Wakti AIØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯ ÙˆÙˆØ¯ÙˆØ¯.'
    : 'You are Wakti AI, a smart, helpful, and friendly assistant.';

  // Apply personalization BEFORE API call for speed
  if (personalTouch) {
    // Add nickname context
    if (personalTouch.nickname) {
      systemPrompt += language === 'ar'
        ? ` Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠÙØ¶Ù„ Ø£Ù† ØªÙ†Ø§Ø¯ÙŠÙ‡ ${personalTouch.nickname}.`
        : ` The user prefers to be called ${personalTouch.nickname}.`;
    }

    // Add AI nickname
    if (personalTouch.aiNickname) {
      systemPrompt += language === 'ar'
        ? ` Ø£Ù†Øª Ù…Ø¹Ø±ÙˆÙ Ø¨Ø§Ø³Ù… ${personalTouch.aiNickname}.`
        : ` You are known as ${personalTouch.aiNickname}.`;
    }

    // Apply tone instructions
    switch (personalTouch.tone) {
      case 'funny':
        systemPrompt += language === 'ar'
          ? ' ÙƒÙ† Ù…Ø±Ø­Ø§Ù‹ ÙˆÙ…Ø³Ù„ÙŠØ§Ù‹ ÙÙŠ Ø±Ø¯ÙˆØ¯Ùƒ.'
          : ' Be funny and entertaining in your responses.';
        break;
      case 'casual':
        systemPrompt += language === 'ar'
          ? ' ØªØ­Ø¯Ø« Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¹Ø§Ø¯ÙŠØ© ÙˆÙ…Ø³ØªØ±Ø®ÙŠØ©.'
          : ' Speak casually and relaxed.';
        break;
      case 'encouraging':
        systemPrompt += language === 'ar'
          ? ' ÙƒÙ† Ù…Ø­ÙØ²Ø§Ù‹ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ§Ù‹ Ø¯Ø§Ø¦Ù…Ø§Ù‹.'
          : ' Be encouraging and positive always.';
        break;
      case 'serious':
        systemPrompt += language === 'ar'
          ? ' ØªØ­Ø¯Ø« Ø¨Ø¬Ø¯ÙŠØ© ÙˆÙ…Ù‡Ù†ÙŠØ©.'
          : ' Speak seriously and professionally.';
        break;
    }

    // Apply style instructions
    switch (personalTouch.style) {
      case 'short answers':
        systemPrompt += language === 'ar'
          ? ' Ø§Ø¬Ø¹Ù„ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ù…Ø®ØªØµØ±Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©.'
          : ' Keep your answers brief and direct.';
        break;
      case 'bullet points':
        systemPrompt += language === 'ar'
          ? ' Ù†Ø¸Ù… Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ ÙÙŠ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø©.'
          : ' Organize your answers in clear bullet points.';
        break;
      case 'step-by-step':
        systemPrompt += language === 'ar'
          ? ' Ø§Ø´Ø±Ø­ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©.'
          : ' Explain things step by step.';
        break;
    }

    // Add custom instructions
    if (personalTouch.instruction && personalTouch.instruction.trim()) {
      systemPrompt += language === 'ar'
        ? ` ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©: ${personalTouch.instruction}`
        : ` Additional instructions: ${personalTouch.instruction}`;
    }
  }

  return systemPrompt;
};

// ULTRA-FAST: Build speed-optimized conversation messages
const buildSpeedOptimizedMessages = (
  userMessage: string, 
  context: string | null, 
  recentMessages: any[], 
  systemPrompt: string,
  interactionType: string
) => {
  const messages = [{ role: 'system', content: systemPrompt }];

  // ULTRA-FAST: Minimal context for speed
  if (context && context.length > 0 && !interactionType.includes('hyper_fast')) {
    messages.push({
      role: 'system',
      content: `Context: ${context.substring(0, 300)}`
    });
  }

  // ULTRA-FAST: Minimal conversation history for maximum speed
  if (recentMessages && recentMessages.length > 0) {
    const maxMessages = interactionType.includes('hyper_fast') ? 1 : 
                       interactionType.includes('ultra_fast') ? 2 : 3;
    
    const conversationHistory = recentMessages.slice(-maxMessages).map(msg => ({
      role: msg.role === 'user' ? 'user' : 'assistant',
      content: typeof msg.content === 'string' 
        ? msg.content.substring(0, interactionType.includes('hyper_fast') ? 100 : 200)
        : '[Message with attachment]'
    }));
    
    messages.push(...conversationHistory);
  }

  // Add current user message
  messages.push({ role: 'user', content: userMessage });

  return messages;
};

// FIXED: Properly working timeout wrapper with AbortController
const makeAPICallWithTimeout = async (
  apiCall: () => Promise<Response>, 
  timeoutMs: number = 12000,
  abortController?: AbortController
): Promise<Response> => {
  
  const controller = abortController || new AbortController();
  const timeoutId = setTimeout(() => {
    console.log(`â° TIMEOUT: Aborting API call after ${timeoutMs}ms`);
    controller.abort();
  }, timeoutMs);
  
  try {
    const response = await apiCall();
    clearTimeout(timeoutId);
    
    if (controller.signal.aborted) {
      throw new Error('Request timeout');
    }
    
    return response;
  } catch (error) {
    clearTimeout(timeoutId);
    
    if (controller.signal.aborted || error.name === 'AbortError') {
      throw new Error('Request timeout');
    }
    
    throw error;
  }
};

// FIXED: Enhanced API call with proper timeout handling and AbortController
const makeResilientAPICall = async (
  messages: any[],
  maxTokens: number,
  temperature: number,
  retryCount: number = 0
): Promise<any> => {
  console.log(`ğŸ”„ API Call Attempt ${retryCount + 1}/3`);
  
  // Try OpenAI first if available
  if (OPENAI_API_KEY) {
    try {
      console.log('ğŸš€ Trying OpenAI with proper timeout handling...');
      
      const openAICall = () => fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${OPENAI_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-4o-mini',
          messages: messages,
          max_tokens: maxTokens,
          temperature: temperature,
          top_p: 0.9,
          frequency_penalty: 0,
          presence_penalty: 0
        }),
      });

      const response = await makeAPICallWithTimeout(openAICall, 12000); // 12 second timeout
      
      if (!response.ok) {
        throw new Error(`OpenAI API failed: ${response.status}`);
      }

      const data = await response.json();
      console.log('âœ… OpenAI Success');
      return { data, provider: 'openai' };
      
    } catch (error) {
      console.warn(`âš ï¸ OpenAI failed: ${error.message}`);
      
      // If it's a timeout or network error and we have retries left, retry
      if (retryCount < 2 && (error.message.includes('timeout') || error.message.includes('network'))) {
        await new Promise(resolve => setTimeout(resolve, Math.pow(2, retryCount) * 1000)); // Exponential backoff
        return makeResilientAPICall(messages, maxTokens, temperature, retryCount + 1);
      }
    }
  }

  // Fallback to DeepSeek
  if (DEEPSEEK_API_KEY) {
    try {
      console.log('ğŸ”„ Falling back to DeepSeek...');
      
      const deepSeekCall = () => fetch('https://api.deepseek.com/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${DEEPSEEK_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'deepseek-chat',
          messages: messages,
          max_tokens: maxTokens,
          temperature: temperature,
          top_p: 0.9
        }),
      });

      const response = await makeAPICallWithTimeout(deepSeekCall, 12000); // 12 second timeout
      
      if (!response.ok) {
        throw new Error(`DeepSeek API failed: ${response.status}`);
      }

      const data = await response.json();
      console.log('âœ… DeepSeek Success');
      return { data, provider: 'deepseek' };
      
    } catch (error) {
      console.warn(`âš ï¸ DeepSeek failed: ${error.message}`);
      
      // If it's a timeout or network error and we have retries left, retry
      if (retryCount < 2 && (error.message.includes('timeout') || error.message.includes('network'))) {
        await new Promise(resolve => setTimeout(resolve, Math.pow(2, retryCount) * 1000)); // Exponential backoff
        return makeResilientAPICall(messages, maxTokens, temperature, retryCount + 1);
      }
    }
  }

  // If all APIs fail, return a proper error instead of generic timeout
  throw new Error('AI services are temporarily busy. Please try again.');
};

// FIXED: Main processing function with proper timeout protection
export async function processWithBuddyChatAI(
  userMessage: string,
  context: string | null = null,
  language: string = 'en',
  recentMessages: any[] = [],
  conversationSummary: string = '',
  activeTrigger: string = 'chat',
  interactionType: string = 'ultra_fast_chat',
  attachedFiles: any[] = [],
  customSystemPrompt: string = '',
  maxTokens: number = 400,
  personalTouch: any | null = null
): Promise<string> {
  
  console.log(`ğŸš€ FIXED TIMEOUT CHAT: Processing with proper timeout handling - ${interactionType} (${maxTokens} tokens)`);
  
  try {
    // ENHANCED: Build personalized system prompt BEFORE API call
    let systemPrompt = customSystemPrompt;
    if (!systemPrompt) {
      systemPrompt = buildPersonalizedSystemPrompt(language, personalTouch, interactionType);
    }

    console.log(`ğŸ¯ PERSONALIZED SYSTEM PROMPT: ${systemPrompt.substring(0, 100)}...`);
    
    // ULTRA-FAST: Minimal context for speed
    let speedContext = context;
    if (interactionType.includes('hyper_fast')) {
      speedContext = null; // No context for maximum speed
    } else if (interactionType.includes('ultra_fast') && context) {
      speedContext = context.substring(0, 200); // Minimal context
    }
    
    // Build speed-optimized conversation messages
    const messages = buildSpeedOptimizedMessages(
      userMessage,
      speedContext,
      recentMessages.slice(-2), // Minimal for speed
      systemPrompt,
      interactionType
    );

    // CRITICAL: Enhanced temperature based on personalization
    let temperature = interactionType.includes('hyper_fast') ? 0.3 : 0.7;
    if (personalTouch?.tone) {
      switch (personalTouch.tone) {
        case 'funny':
          temperature = Math.min(temperature + 0.2, 0.9);
          break;
        case 'serious':
          temperature = Math.max(temperature - 0.2, 0.3);
          break;
        case 'casual':
          temperature = Math.min(temperature + 0.1, 0.8);
          break;
      }
    }

    console.log(`ğŸ¯ PERSONALIZED TEMPERATURE: ${temperature}`);

    // FIXED: Make resilient API call with proper timeout handling
    const result = await makeResilientAPICall(messages, maxTokens, temperature);
    const aiResponse = result.data.choices[0].message.content;
    
    console.log(`âœ… SUCCESS via ${result.provider.toUpperCase()}: ${aiResponse.length} characters`);
    
    return aiResponse;
    
  } catch (error) {
    console.error('ğŸš¨ FIXED TIMEOUT ERROR:', error);
    
    // IMPROVED: Return user-friendly messages instead of technical errors
    if (error.message?.includes('timeout') || error.message?.includes('busy')) {
      const fallbackResponse = language === 'ar' 
        ? 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ù…Ø¹ØªØ§Ø¯. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù…Ù† ÙØ¶Ù„Ùƒ.'
        : 'Sorry, AI is taking longer than usual. Please try again.';
      
      return fallbackResponse;
    }
    
    // Generic friendly fallback
    const fallbackResponse = language === 'ar' 
      ? 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø£ÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø© Ù…Ø¤Ù‚ØªØ©. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù…Ù† ÙØ¶Ù„Ùƒ.'
      : 'Sorry, I\'m having a temporary issue. Please try again.';
    
    return fallbackResponse;
  }
}
