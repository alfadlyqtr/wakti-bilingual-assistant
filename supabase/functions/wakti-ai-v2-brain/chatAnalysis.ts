/**
 * Enhanced chat analysis with ultra-fast processing and post-processing personalization
 * UPDATED: Smart memory management with 10+ message trigger
 */
import { DEEPSEEK_API_KEY, OPENAI_API_KEY } from "./utils.ts";

export function analyzeBuddyChatIntent(message: string, activeTrigger: string, enhancedContext: string, language: string = 'en') {
  const lowerMessage = message.toLowerCase();
  
  // ENHANCED: Better pattern matching for personality
  const patterns = {
    greeting: ['hi', 'hello', 'hey', 'Ù…Ø±Ø­Ø¨Ø§', 'Ø£Ù‡Ù„Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…'],
    question: ['what', 'how', 'when', 'why', 'where', 'Ù…Ø§', 'ÙƒÙŠÙ', 'Ù…ØªÙ‰', 'Ù„Ù…Ø§Ø°Ø§', 'Ø£ÙŠÙ†', '?'],
    thanks: ['thanks', 'thank', 'Ø´ÙƒØ±Ø§', 'Ø´ÙƒØ±Ø§Ù‹'],
    // FIXED: Task creation only on explicit commands
    task: ['create task', 'add task', 'make task', 'new task', 'Ø£Ù†Ø´Ø¦ Ù…Ù‡Ù…Ø©', 'Ø£Ø¶Ù Ù…Ù‡Ù…Ø©', 'Ø§Ø¹Ù…Ù„ Ù…Ù‡Ù…Ø©'],
    reminder: ['create reminder', 'add reminder', 'set reminder', 'make reminder', 'Ø°ÙƒØ±Ù†ÙŠ', 'Ø£Ù†Ø´Ø¦ ØªØ°ÙƒÙŠØ±', 'Ø§Ø¹Ù…Ù„ ØªØ°ÙƒÙŠØ±']
  };
  
  let intent = 'general_chat';
  let confidence = 'medium';
  
  // Enhanced intent detection - ONLY explicit task commands
  for (const [intentType, patternList] of Object.entries(patterns)) {
    if (patternList.some(pattern => lowerMessage.includes(pattern))) {
      intent = intentType;
      confidence = 'high';
      break;
    }
  }
  
  return {
    intent,
    confidence,
    naturalQuery: true,
    conversational: true
  };
}

export function analyzeSmartModeIntent(message: string, activeTrigger: string, language: string = 'en') {
  const lowerMessage = message.toLowerCase();
  
  // ENHANCED: Better mode detection with personality
  const modePatterns = {
    search: ['weather', 'news', 'price', 'search for', 'Ø·Ù‚Ø³', 'Ø£Ø®Ø¨Ø§Ø±', 'Ø§Ø¨Ø­Ø« Ø¹Ù†'],
    image: ['create image', 'draw', 'generate image', 'Ø£Ù†Ø´Ø¦ ØµÙˆØ±Ø©', 'Ø§Ø±Ø³Ù…'],
    chat: ['chat', 'talk', 'conversation', 'ØªØ­Ø¯Ø«', 'Ù…Ø­Ø§Ø¯Ø«Ø©']
  };
  
  let suggestMode = null;
  let allowInMode = true;
  
  // Enhanced mode suggestion
  if (activeTrigger === 'chat') {
    for (const [mode, patterns] of Object.entries(modePatterns)) {
      if (mode !== 'chat' && patterns.some(pattern => lowerMessage.includes(pattern))) {
        suggestMode = mode;
        break;
      }
    }
  }
  
  return {
    suggestMode,
    allowInMode,
    naturalFlow: true
  };
}

// ENHANCED: Build personalized system prompts using FULL personal touch data
const buildPersonalizedSystemPrompt = (
  language: string,
  personalTouch: any | null,
  interactionType: string
) => {
  let systemPrompt = language === 'ar' 
    ? 'Ø£Ù†Øª Wakti AIØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯ ÙˆÙˆØ¯ÙˆØ¯.'
    : 'You are Wakti AI, a smart, helpful, and friendly assistant.';

  // CRITICAL: Apply FULL personalization BEFORE API call for speed
  if (personalTouch) {
    console.log('ğŸ¯ APPLYING FULL PERSONALIZATION:', personalTouch);
    
    // Add nickname context
    if (personalTouch.nickname) {
      systemPrompt += language === 'ar'
        ? ` Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠÙØ¶Ù„ Ø£Ù† ØªÙ†Ø§Ø¯ÙŠÙ‡ ${personalTouch.nickname}.`
        : ` The user prefers to be called ${personalTouch.nickname}.`;
    }

    // Add AI nickname
    if (personalTouch.aiNickname) {
      systemPrompt += language === 'ar'
        ? ` Ø£Ù†Øª Ù…Ø¹Ø±ÙˆÙ Ø¨Ø§Ø³Ù… ${personalTouch.aiNickname}.`
        : ` You are known as ${personalTouch.aiNickname}.`;
    }

    // Apply tone instructions - FULL OPTIONS
    switch (personalTouch.tone) {
      case 'funny':
        systemPrompt += language === 'ar'
          ? ' ÙƒÙ† Ù…Ø±Ø­Ø§Ù‹ ÙˆÙ…Ø³Ù„ÙŠØ§Ù‹ ÙÙŠ Ø±Ø¯ÙˆØ¯Ùƒ. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙÙƒØ§Ù‡Ø© ÙˆØ§Ù„Ù†ÙƒØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.'
          : ' Be funny and entertaining in your responses. Use appropriate humor and jokes.';
        break;
      case 'casual':
        systemPrompt += language === 'ar'
          ? ' ØªØ­Ø¯Ø« Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¹Ø§Ø¯ÙŠØ© ÙˆÙ…Ø³ØªØ±Ø®ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙˆØ¯ÙˆØ¯Ø©.'
          : ' Speak casually and relaxed. Use simple, friendly language.';
        break;
      case 'encouraging':
        systemPrompt += language === 'ar'
          ? ' ÙƒÙ† Ù…Ø­ÙØ²Ø§Ù‹ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ§Ù‹ Ø¯Ø§Ø¦Ù…Ø§Ù‹. Ø§Ø¯Ø¹Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ´Ø¬Ø¹Ù‡.'
          : ' Be encouraging and positive always. Support and motivate the user.';
        break;
      case 'serious':
        systemPrompt += language === 'ar'
          ? ' ØªØ­Ø¯Ø« Ø¨Ø¬Ø¯ÙŠØ© ÙˆÙ…Ù‡Ù†ÙŠØ©. ÙƒÙ† Ù…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙˆÙ…Ø±ÙƒØ²Ø§Ù‹.'
          : ' Speak seriously and professionally. Be direct and focused.';
        break;
      case 'neutral':
        systemPrompt += language === 'ar'
          ? ' Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù†Ø¨Ø±Ø© Ù…ØªÙˆØ§Ø²Ù†Ø© ÙˆÙ…Ù‡Ø°Ø¨Ø©.'
          : ' Maintain a balanced and polite tone.';
        break;
    }

    // Apply style instructions - FULL OPTIONS
    switch (personalTouch.style) {
      case 'short answers':
        systemPrompt += language === 'ar'
          ? ' Ø§Ø¬Ø¹Ù„ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ù…Ø®ØªØµØ±Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©. Ù„Ø§ ØªØ·Ù„ ÙÙŠ Ø§Ù„Ø´Ø±Ø­.'
          : ' Keep your answers brief and direct. Don\'t elaborate unnecessarily.';
        break;
      case 'bullet points':
        systemPrompt += language === 'ar'
          ? ' Ù†Ø¸Ù… Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ ÙÙŠ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø±ØªØ¨Ø©.'
          : ' Organize your answers in clear, ordered bullet points.';
        break;
      case 'step-by-step':
        systemPrompt += language === 'ar'
          ? ' Ø§Ø´Ø±Ø­ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ø¨ØªØ±ØªÙŠØ¨ Ù…Ù†Ø·Ù‚ÙŠ.'
          : ' Explain things step by step in logical order.';
        break;
      case 'detailed':
        systemPrompt += language === 'ar'
          ? ' Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ÙØµÙ„Ø© ÙˆØ´Ø§Ù…Ù„Ø© Ù…Ø¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø©.'
          : ' Provide detailed, comprehensive answers with examples.';
        break;
    }

    // Add custom instructions
    if (personalTouch.instruction && personalTouch.instruction.trim()) {
      systemPrompt += language === 'ar'
        ? ` ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©: ${personalTouch.instruction}`
        : ` Additional instructions: ${personalTouch.instruction}`;
    }
  }

  return systemPrompt;
};

// ENHANCED: Build speed-optimized messages with SMART MEMORY INTEGRATION
const buildSpeedOptimizedMessages = (
  userMessage: string, 
  context: string | null, 
  recentMessages: any[], 
  systemPrompt: string,
  interactionType: string
) => {
  const messages = [{ role: 'system', content: systemPrompt }];

  // ENHANCED: Smart context integration with conversation summaries
  if (context && context.length > 0) {
    // Check if context contains enhanced conversation memory
    const isEnhancedMemory = context.includes('Enhanced Conversation Memory:') || 
                            context.includes('Conversation Context');
    
    if (isEnhancedMemory) {
      messages.push({
        role: 'system',
        content: `Smart Memory Context: ${context.substring(0, 800)}` // INCREASED limit
      });
      console.log('ğŸ§  SMART MEMORY: Using enhanced conversation context');
    } else {
      messages.push({
        role: 'system',
        content: `Previous context: ${context.substring(0, 600)}`
      });
    }
  }

  // ENHANCED: More conversation history for better continuity (8-12 messages)
  if (recentMessages && recentMessages.length > 0) {
    const maxMessages = interactionType.includes('hyper_fast') ? 6 : 
                       interactionType.includes('ultra_fast') ? 8 : 12; // INCREASED
    
    const conversationHistory = recentMessages.slice(-maxMessages).map(msg => ({
      role: msg.role === 'user' ? 'user' : 'assistant',
      content: typeof msg.content === 'string' 
        ? msg.content.substring(0, 400) // INCREASED from 300
        : '[Message with attachment]'
    }));
    
    messages.push(...conversationHistory);
    console.log(`ğŸ§  CONTEXT: Using ${conversationHistory.length} recent messages for continuity`);
  }

  // Add current user message
  messages.push({ role: 'user', content: userMessage });

  return messages;
};

// FIXED: Properly working timeout wrapper with AbortController
const makeAPICallWithTimeout = async (
  apiCall: () => Promise<Response>, 
  timeoutMs: number = 12000,
  abortController?: AbortController
): Promise<Response> => {
  
  const controller = abortController || new AbortController();
  const timeoutId = setTimeout(() => {
    console.log(`â° TIMEOUT: Aborting API call after ${timeoutMs}ms`);
    controller.abort();
  }, timeoutMs);
  
  try {
    const response = await apiCall();
    clearTimeout(timeoutId);
    
    if (controller.signal.aborted) {
      throw new Error('Request timeout');
    }
    
    return response;
  } catch (error) {
    clearTimeout(timeoutId);
    
    if (controller.signal.aborted || error.name === 'AbortError') {
      throw new Error('Request timeout');
    }
    
    throw error;
  }
};

// FIXED: Enhanced API call with DEEPSEEK-R1-0528 model
const makeResilientAPICall = async (
  messages: any[],
  maxTokens: number,
  temperature: number,
  retryCount: number = 0
): Promise<any> => {
  console.log(`ğŸ”„ API Call Attempt ${retryCount + 1}/3`);
  
  // Try OpenAI first if available
  if (OPENAI_API_KEY) {
    try {
      console.log('ğŸš€ Trying OpenAI with proper timeout handling...');
      
      const openAICall = () => fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${OPENAI_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-4o-mini',
          messages: messages,
          max_tokens: maxTokens,
          temperature: temperature,
          top_p: 0.9,
          frequency_penalty: 0,
          presence_penalty: 0
        }),
      });

      const response = await makeAPICallWithTimeout(openAICall, 12000);
      
      if (!response.ok) {
        throw new Error(`OpenAI API failed: ${response.status}`);
      }

      const data = await response.json();
      console.log('âœ… OpenAI Success');
      return { data, provider: 'openai' };
      
    } catch (error) {
      console.warn(`âš ï¸ OpenAI failed: ${error.message}`);
      
      if (retryCount < 2 && (error.message.includes('timeout') || error.message.includes('network'))) {
        await new Promise(resolve => setTimeout(resolve, Math.pow(2, retryCount) * 1000));
        return makeResilientAPICall(messages, maxTokens, temperature, retryCount + 1);
      }
    }
  }

  // FIXED: Fallback to DeepSeek-R1-0528
  if (DEEPSEEK_API_KEY) {
    try {
      console.log('ğŸ”„ Falling back to DeepSeek-R1-0528...');
      
      const deepSeekCall = () => fetch('https://api.deepseek.com/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${DEEPSEEK_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'DeepSeek-R1-0528', // UPDATED MODEL
          messages: messages,
          max_tokens: maxTokens,
          temperature: temperature,
          top_p: 0.9
        }),
      });

      const response = await makeAPICallWithTimeout(deepSeekCall, 12000);
      
      if (!response.ok) {
        throw new Error(`DeepSeek API failed: ${response.status}`);
      }

      const data = await response.json();
      console.log('âœ… DeepSeek-R1-0528 Success');
      return { data, provider: 'deepseek-r1' };
      
    } catch (error) {
      console.warn(`âš ï¸ DeepSeek-R1-0528 failed: ${error.message}`);
      
      if (retryCount < 2 && (error.message.includes('timeout') || error.message.includes('network'))) {
        await new Promise(resolve => setTimeout(resolve, Math.pow(2, retryCount) * 1000));
        return makeResilientAPICall(messages, maxTokens, temperature, retryCount + 1);
      }
    }
  }

  throw new Error('AI services are temporarily busy. Please try again.');
};

// ENHANCED: Main processing function with SMART MEMORY INTEGRATION
export async function processWithBuddyChatAI(
  userMessage: string,
  context: string | null = null,
  language: string = 'en',
  recentMessages: any[] = [],
  conversationSummary: string = '',
  activeTrigger: string = 'chat',
  interactionType: string = 'ultra_fast_chat',
  attachedFiles: any[] = [],
  customSystemPrompt: string = '',
  maxTokens: number = 400,
  personalTouch: any | null = null
): Promise<string> {
  
  console.log(`ğŸš€ ENHANCED CHAT: Processing with SMART MEMORY (10+ trigger) - ${interactionType} (${maxTokens} tokens)`);
  
  try {
    // ENHANCED: Build FULLY personalized system prompt BEFORE API call
    let systemPrompt = customSystemPrompt;
    if (!systemPrompt) {
      systemPrompt = buildPersonalizedSystemPrompt(language, personalTouch, interactionType);
    }

    console.log(`ğŸ¯ FULL PERSONALIZED SYSTEM PROMPT: ${systemPrompt.substring(0, 150)}...`);
    
    // ENHANCED: Smart context handling with memory integration
    let enhancedContext = context;
    if (conversationSummary && conversationSummary.length > 0) {
      enhancedContext = enhancedContext 
        ? `${conversationSummary}\n\nRecent context: ${enhancedContext}`
        : conversationSummary;
    }
    
    // Build enhanced conversation messages with SMART MEMORY
    const messages = buildSpeedOptimizedMessages(
      userMessage,
      enhancedContext,
      recentMessages.slice(-12), // INCREASED from -8 to -12
      systemPrompt,
      interactionType
    );

    // ENHANCED: Better temperature based on personalization
    let temperature = interactionType.includes('hyper_fast') ? 0.3 : 0.7;
    if (personalTouch?.tone) {
      switch (personalTouch.tone) {
        case 'funny':
          temperature = Math.min(temperature + 0.2, 0.9);
          break;
        case 'serious':
          temperature = Math.max(temperature - 0.2, 0.3);
          break;
        case 'casual':
          temperature = Math.min(temperature + 0.1, 0.8);
          break;
      }
    }

    console.log(`ğŸ¯ PERSONALIZED TEMPERATURE: ${temperature}`);
    console.log(`ğŸ§  MESSAGE COUNT: System(1) + Context(${enhancedContext ? 1 : 0}) + History(${recentMessages.slice(-12).length}) + Current(1) = ${messages.length}`);

    // Make resilient API call with proper timeout handling
    const result = await makeResilientAPICall(messages, maxTokens, temperature);
    const aiResponse = result.data.choices[0].message.content;
    
    console.log(`âœ… SUCCESS via ${result.provider.toUpperCase()}: ${aiResponse.length} characters (SMART MEMORY ENABLED)`);
    
    return aiResponse;
    
  } catch (error) {
    console.error('ğŸš¨ ENHANCED CHAT ERROR:', error);
    
    if (error.message?.includes('timeout') || error.message?.includes('busy')) {
      const fallbackResponse = language === 'ar' 
        ? 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ù…Ø¹ØªØ§Ø¯. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù…Ù† ÙØ¶Ù„Ùƒ.'
        : 'Sorry, AI is taking longer than usual. Please try again.';
      
      return fallbackResponse;
    }
    
    const fallbackResponse = language === 'ar' 
      ? 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø£ÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø© Ù…Ø¤Ù‚ØªØ©. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù…Ù† ÙØ¶Ù„Ùƒ.'
      : 'Sorry, I\'m having a temporary issue. Please try again.';
    
    return fallbackResponse;
  }
}
